PRACTICAL NO.10

A] Simple Neutral Network
CODE:
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn .model_selection import train_test_split
from sklearn import metrics
from matplotlib import pyplot as plt
from sklearn import tree

col_names=['Reservation','Raining','BadService','Satur','Result']
hoteldata = pd.read_csv("dtree.csv",header=None,names=col_names)
feature_cols=['Reservation','Raining','BadService','Satur']
X=hoteldata[feature_cols]
y=hoteldata.Result
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)
print(hoteldata)

print("x train data:", X_train)
print("y train data:",y_train)
print("x test data:",X_test)
print("y test data:",y_test)
clf=DecisionTreeClassifier(criterion="entropy",max_depth=5)
clf=clf.fit(X_train, y_train)
y_pred=clf.predict(X_test)
print("ytest=",X_test)
print("ypred=",y_pred)
print("Accuracy:",metrics.accuracy_score(y_test,y_pred))
fig=plt.figure(figsize=(25,20))
t=tree.plot_tree(clf,feature_names=feature_cols,class_names=['Leave','wait'], filled=True)
fig.savefig("decistion_tree.png")


xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

B] Support Vector Machine
Import matplotlib.pyplot as plt

From sklearn import svm

From sklearn.datasets import load_iris

From sklearn.model_selection import train_test_split

From sklearn import metrics

Iris=load_iris()

Print(iris.data)

Print(“Feature name: “,iris.feature_names)

Print(“Labels: “,iris.target_names)

X_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.4,random_state=1)

Classifier=svm.SVC(kernel=’linear’)

Print(“X train: “,x_train)

Print(“Y train: “,y_train)

Classifier.fit(x_train,y_train)

Print(“Test data: “,x_test)

Y_pred=classifier.predict(x_test)

#print(“Predicted labels: “, y_pred)

Print(“Accuracy:”, metrics.accuracy_score(y_test, y_pred))

#print(“Classification Report:\n”, metrics.classification_report(y_test, y_pred))

Print(“Confusion Matrix:\n”, metrics.confusion_matrix(y_test, y_pred))

Iris=load_iris()

X=iris.data[:,:2]

Y=iris.target

Plt.scatter(x[:,0],x[:,1],c=y,cmap=plt.cm.coolwarm)

Plt.xlabel(‘Sepal length’)

Plt.ylabel(‘Sepal Width’)

Plt.title(‘Sepal Width& Length’)

Plt.show()

With Confusion Matrix:

Import numpy as np

Import matplotlib.pyplot as plt

From sklearn import svm

From sklearn.datasets import load_iris

From sklearn.model_selection import train_test_split

From sklearn import metrics

From sklearn.metrics import ConfusionMatrixDisplay

# Load data

Iris = load_iris()

Print(iris.data)

Print(“Feature name: “, iris.feature_names)

Print(“Labels: “, iris.target_names)

# Split data

X_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=1)

# Create and train classifier

Classifier = svm.SVC(kernel=’linear’)

Print(“X train: “, x_train)

Print(“Y train: “, y_train)

Classifier.fit(x_train, y_train)

# Predict and evaluate

Print(“Test data: “, x_test)

Y_pred = classifier.predict(x_test)

Print(“Accuracy:”, metrics.accuracy_score(y_test, y_pred))

Print(“Confusion Matrix:\n”, metrics.confusion_matrix(y_test, y_pred))

# Create a figure with 1 row and 2 columns

Fig, axs = plt.subplots(1, 2, figsize=(14, 6))

# Plot confusion matrix on the first subplot

ConfusionMatrixDisplay.from_estimator(classifier, x_test, y_test, cmap=plt.cm.Blues, ax=axs[0])

Axs[0].set_title(‘Confusion Matrix’)

# Use only first two features for plotting on second subplot

X = iris.data[:, :2]

Y = iris.target

Colors = [‘red’, ‘green’, ‘blue’]

# Plot scatter points and connecting lines per class

For class_idx, color in enumerate(colors):

    Class_points = x[y == class_idx]

    Sorted_idx = np.argsort(class_points[:, 0])

    Class_points_sorted = class_points[sorted_idx]

    # Scatter dots

    Axs[1].scatter(class_points[:, 0], class_points[:, 1], color=color, edgecolors=’k’, label=f’{iris.target_names[class_idx]} points’)

    # Connecting line

    Axs[1].plot(class_points_sorted[:, 0], class_points_sorted[:, 1], color=color, alpha=0.6)

Axs[1].set_xlabel(‘Sepal length’)

Axs[1].set_ylabel(‘Sepal width’)

Axs[1].set_title(‘Sepal Width & Length with Scatter and Lines’)

# Train SVM on first two features for decision boundaries

Clf_2d = svm.SVC(kernel=’linear’)

Clf_2d.fit(x, y)

W = clf_2d.coef_

B = clf_2d.intercept_

X_min, x_max = x[:, 0].min() – 0.5, x[:, 0].max() + 0.5

X_plot = np.linspace(x_min, x_max, 200)

For I in range(len(w)):

    Y_plot = -(w[i][0] / w[i][1]) * x_plot – b[i] / w[i][1]

    Axs[1].plot(x_plot, y_plot, color=colors[i], linestyle=’—‘, label=f’Class {iris.target_names[i]} boundary’)

Axs[1].legend()

Plt.tight_layout()

Plt.show()
