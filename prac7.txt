PRACTICAL 7 : 
--------------------------------------------------------

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser
import time
import random

class SmartCrawler:
    def __init__(self, base_url, max_pages=10, user_agent='MyCrawler'):
        self.base_url = base_url
        self.visited = set()
        self.to_visit = [base_url]
        self.max_pages = max_pages
        self.user_agent = user_agent
        self.index = {}
        self.robot_parser = self.setup_robot_parser(base_url)

    def setup_robot_parser(self, url):
        parsed_url = urlparse(url)
        robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
        rp = RobotFileParser()
        rp.set_url(robots_url)
        try:
            rp.read()
        except:
            pass
        return rp

    def is_allowed(self, url):
        try:
            return self.robot_parser.can_fetch(self.user_agent, url)
        except:
            return False

    def polite_delay(self, min_delay=1, max_delay=3):
        delay = random.uniform(min_delay, max_delay)
        print(f"Waiting for {delay:.2f} seconds")
        time.sleep(delay)

    def fetch_html(self, url):
        try:
            headers = {'User-Agent': self.user_agent}
            response = requests.get(url, headers=headers, timeout=5)
            if "text/html" in response.headers.get("Content-Type", ""):
                return response.text
        except Exception as e:
            print(f"Error fetching {url}: {e}")
        return None

    def crawl(self):
        while self.to_visit and len(self.visited) < self.max_pages:
            url = self.to_visit.pop(0)
            if url in self.visited or not self.is_allowed(url):
                continue

            print(f"Crawling: {url}")
            html = self.fetch_html(url)
            if not html:
                continue

            soup = BeautifulSoup(html, 'html.parser')
            self.visited.add(url)
            self.index_page(url, soup)

            for link in soup.find_all('a', href=True):
                full_url = urljoin(url, link['href'])
                if self.is_same_domain(full_url) and full_url not in self.visited:
                    self.to_visit.append(full_url)

            self.polite_delay()

    def is_same_domain(self, url):
        return urlparse(url).netloc == urlparse(self.base_url).netloc

    def index_page(self, url, soup):
        title = soup.title.string.strip() if soup.title else 'No Title'
        text = soup.get_text(separator=' ', strip=True)
        self.index[url] = {
            'title': title,
            'snippet': text[:200]
        }

    def print_index(self):
        for url, data in self.index.items():
            print(f"\nURL: {url}")
            print(f"Title: {data['title']}")
            print(f"Snippet: {data['snippet']}...")

# Example usage
if __name__ == "__main__":
    crawler = SmartCrawler("https://www.hotstar.com", max_pages=5)
    crawler.crawl()
    crawler.print_index()

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time

class WebCrawler:
    def __init__(self, base_url, max_pages=50):
        self.base_url = base_url
        self.visited = set()
        self.to_visit = [base_url]
        self.max_pages = max_pages
        self.index = {}

    def crawl(self):
        while self.to_visit and len(self.visited) < self.max_pages:
            url = self.to_visit.pop(0)
            if url in self.visited:
                continue

            print(f"Crawling: {url}")
            try:
                response = requests.get(url, timeout=5)
                if "text/html" not in response.headers.get("Content-Type", ""):
                    continue
            except Exception as e:
                print(f"Failed to crawl {url}: {e}")
                continue

            self.visited.add(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            self.index_page(url, soup)

            for link in soup.find_all('a', href=True):
                full_url = urljoin(url, link['href'])
                if self.is_valid_url(full_url):
                    if full_url not in self.visited and full_url not in self.to_visit:
                        self.to_visit.append(full_url)

            time.sleep(1)  # polite crawling

    def is_valid_url(self, url):
        parsed = urlparse(url)
        return parsed.scheme in {"http", "https"} and parsed.netloc == urlparse(self.base_url).netloc

    def index_page(self, url, soup):
        title = soup.title.string.strip() if soup.title else 'No title'
        text = soup.get_text(separator=' ', strip=True)
        self.index[url] = {
            'title': title,
            'content_snippet': text[:200]  # Just store a snippet for now
        }

    def print_index(self):
        for url, data in self.index.items():
            print(f"\nURL: {url}")
            print(f"Title: {data['title']}")
            print(f"Snippet: {data['content_snippet']}...")

# Example usage
if __name__ == "__main__":
    start_url = "https://www.hotstar.com/"  # Replace with your target URL
    crawler = WebCrawler(start_url, max_pages=10)
    crawler.crawl()
    crawler.print_index()

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX